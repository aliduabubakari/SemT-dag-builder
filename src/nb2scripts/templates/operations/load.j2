def run_load_operation_{{ index }}(config: Config, file_manager: PipelineFileManager):
    """
    LOAD Operation: {{ op.name }}
    """
    logger.info("=== LOAD OPERATION: {{ op.name }} ===")

    # Initialize managers
    auth_manager = AuthManager(config.auth_api_url, config.username, config.password)
    table_manager = TableManager(config.table_base_url, auth_manager)

    # Read CSV
    csv_path = config.input_file_path
    if not csv_path or not os.path.exists(csv_path):
        raise FileNotFoundError(f"Input CSV not found: {csv_path}")

    logger.info(f"ğŸ“‚ Reading CSV: {csv_path}")
    df = pd.read_csv(csv_path)

    {% if params.columns_to_delete %}
    # Delete columns
    columns_to_delete = {{ params.columns_to_delete | tojson }}
    for col in columns_to_delete:
        if col in df.columns:
            df = df.drop(columns=[col])
            logger.info(f"   Deleted column: {col}")
    {% endif %}

    # Determine table name (env > param > csv filename)
    table_name = (config.table_name or "{{ params.table_name | default('') }}").strip()
    if not table_name:
        base = os.path.basename(csv_path)
        table_name = os.path.splitext(base)[0]
    logger.info(f"â¬†ï¸  Uploading table: {table_name}")

    # Upload to backend
    table_id, message, response_data = table_manager.add_table(
        config.dataset_id, df, table_name
    )

    if not table_id:
        raise RuntimeError(f"Failed to upload table: {message}")

    logger.info(f"âœ… Table created with ID: {table_id}")

    # Retrieve full table data
    table_data = table_manager.get_table(config.dataset_id, table_id)

    # Save state locally (JSON for next stage)
    output_path = file_manager.save_current_state(table_data)
    file_manager.save_stage_snapshot("load", table_data, config.stage_number)

    logger.info(f"ğŸ’¾ Table data saved to: {output_path}")
    return table_data